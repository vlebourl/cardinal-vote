"""End-to-end workflow validation tests for the ToVÃ©Co voting platform.

This module tests the complete user journey:
- Full voting workflow from start to finish
- Cross-system integration validation
- User experience validation
- Data flow verification
- Business logic validation
"""

import time
from typing import Any

import pytest
from fastapi.testclient import TestClient

from src.toveco_voting.main import app
from tests.test_fixtures import (
    TestScenarios,
)


class WorkflowValidator:
    """Validates complete end-to-end workflows."""

    def __init__(self, client: TestClient):
        self.client = client
        self.workflow_state = {}

    def reset_workflow(self):
        """Reset workflow state."""
        self.workflow_state = {}

    def validate_step(self, step_name: str, condition: bool, message: str = ""):
        """Validate a workflow step."""
        self.workflow_state[step_name] = {
            "passed": condition,
            "message": message,
            "timestamp": time.time(),
        }

        if not condition:
            raise AssertionError(f"Workflow step '{step_name}' failed: {message}")

    def get_workflow_summary(self) -> dict[str, Any]:
        """Get summary of workflow validation."""
        total_steps = len(self.workflow_state)
        passed_steps = sum(1 for step in self.workflow_state.values() if step["passed"])

        return {
            "total_steps": total_steps,
            "passed_steps": passed_steps,
            "success_rate": (passed_steps / total_steps * 100)
            if total_steps > 0
            else 0,
            "steps": self.workflow_state,
        }


class TestCompleteVotingWorkflow:
    """Test the complete voting workflow from start to finish."""

    @pytest.fixture
    def client(self):
        """Create test client."""
        with TestClient(app) as test_client:
            yield test_client

    @pytest.fixture
    def workflow_validator(self, client):
        """Create workflow validator."""
        return WorkflowValidator(client)

    def test_single_user_complete_workflow(self, workflow_validator):
        """Test complete workflow for a single user."""
        validator = workflow_validator
        validator.reset_workflow()

        # Step 1: Access homepage
        response = validator.client.get("/")
        validator.validate_step(
            "homepage_access",
            response.status_code == 200,
            f"Homepage returned status {response.status_code}",
        )

        # Step 2: Get available logos
        response = validator.client.get("/api/logos")
        if response.status_code == 200:
            logos_data = response.json()
            logos = logos_data.get("logos", [])

            validator.validate_step(
                "logos_retrieval",
                len(logos) == 11,
                f"Expected 11 logos, got {len(logos)}",
            )
        else:
            # Skip if logos not available in test environment
            validator.validate_step(
                "logos_retrieval",
                True,
                "Skipped - logos not available in test environment",
            )
            logos = [f"toveco{i}.png" for i in range(1, 12)]

        # Step 3: Prepare vote data
        vote_data = {
            "voter_name": "E2E Test User",
            "ratings": {logo: (hash(logo) % 5) - 2 for logo in logos},
        }

        validator.validate_step(
            "vote_preparation",
            len(vote_data["ratings"]) == 11,
            f"Vote data prepared for {len(vote_data['ratings'])} logos",
        )

        # Step 4: Submit vote
        response = validator.client.post("/api/vote", json=vote_data)

        if response.status_code == 200:
            vote_response = response.json()
            validator.validate_step(
                "vote_submission",
                vote_response.get("success") is True,
                f"Vote submission response: {vote_response.get('message', 'No message')}",
            )

            vote_id = vote_response.get("vote_id")
            validator.validate_step(
                "vote_id_received",
                vote_id is not None and vote_id > 0,
                f"Received vote ID: {vote_id}",
            )
        else:
            # Handle case where vote submission fails in test environment
            validator.validate_step(
                "vote_submission",
                response.status_code in [400, 422],
                f"Vote submission failed as expected in test env: {response.status_code}",
            )

        # Step 5: Verify results can be retrieved
        response = validator.client.get("/api/results")
        validator.validate_step(
            "results_retrieval",
            response.status_code == 200,
            f"Results retrieval status: {response.status_code}",
        )

        if response.status_code == 200:
            results_data = response.json()
            validator.validate_step(
                "results_structure",
                "summary" in results_data and "total_voters" in results_data,
                "Results have expected structure",
            )

        # Step 6: Access results page
        response = validator.client.get("/results")
        validator.validate_step(
            "results_page_access",
            response.status_code == 200,
            f"Results page status: {response.status_code}",
        )

        # Workflow summary
        summary = validator.get_workflow_summary()
        print("\nSingle User Workflow Summary:")
        print(f"  Steps passed: {summary['passed_steps']}/{summary['total_steps']}")
        print(f"  Success rate: {summary['success_rate']:.1f}%")

        assert summary["success_rate"] >= 80, (
            f"Workflow success rate too low: {summary['success_rate']:.1f}%"
        )

    def test_multiple_users_workflow(self, workflow_validator):
        """Test workflow with multiple users voting."""
        validator = workflow_validator
        validator.reset_workflow()

        # Use predefined test scenario
        voting_session = TestScenarios.small_voting_session()

        validator.validate_step(
            "test_data_preparation",
            len(voting_session) == 5,
            f"Prepared {len(voting_session)} test votes",
        )

        successful_votes = 0
        vote_ids = []

        # Submit votes for all users
        for i, vote_data in enumerate(voting_session):
            response = validator.client.post("/api/vote", json=vote_data)

            step_name = f"vote_submission_user_{i + 1}"

            if response.status_code == 200:
                vote_response = response.json()
                if vote_response.get("success"):
                    successful_votes += 1
                    vote_ids.append(vote_response.get("vote_id"))

                    validator.validate_step(
                        step_name,
                        True,
                        f"User {vote_data['voter_name']} vote successful",
                    )
                else:
                    validator.validate_step(
                        step_name,
                        False,
                        f"Vote failed for {vote_data['voter_name']}: {vote_response.get('message')}",
                    )
            else:
                # Accept certain failures in test environment
                validator.validate_step(
                    step_name,
                    response.status_code in [400, 422],
                    f"Vote submission failed as expected: {response.status_code}",
                )

        # Validate that at least some votes succeeded
        validator.validate_step(
            "multiple_votes_success",
            successful_votes > 0,
            f"{successful_votes} out of {len(voting_session)} votes succeeded",
        )

        # Get and validate results
        response = validator.client.get("/api/results")
        if response.status_code == 200 and successful_votes > 0:
            results_data = response.json()

            validator.validate_step(
                "results_after_multiple_votes",
                results_data.get("total_voters") == successful_votes,
                f"Results show {results_data.get('total_voters')} voters (expected {successful_votes})",
            )

            # Validate results structure
            summary = results_data.get("summary", {})
            validator.validate_step(
                "results_summary_structure",
                len(summary) > 0,
                f"Results summary contains {len(summary)} logos",
            )

            # Validate ranking logic
            if summary:
                rankings = [stats["ranking"] for stats in summary.values()]
                unique_rankings = len(set(rankings))

                validator.validate_step(
                    "ranking_logic",
                    unique_rankings == len(rankings),  # All rankings should be unique
                    f"Rankings are properly assigned: {sorted(rankings)}",
                )

        # Workflow summary
        summary = validator.get_workflow_summary()
        print("\nMultiple Users Workflow Summary:")
        print(f"  Steps passed: {summary['passed_steps']}/{summary['total_steps']}")
        print(f"  Success rate: {summary['success_rate']:.1f}%")
        print(f"  Successful votes: {successful_votes}/{len(voting_session)}")

        assert summary["success_rate"] >= 70, (
            f"Multi-user workflow success rate too low: {summary['success_rate']:.1f}%"
        )

    def test_edge_case_workflow(self, workflow_validator):
        """Test workflow with edge cases and error conditions."""
        validator = workflow_validator
        validator.reset_workflow()

        # Test 1: Empty voter name
        response = validator.client.post(
            "/api/vote", json={"voter_name": "", "ratings": {"toveco1.png": 1}}
        )

        validator.validate_step(
            "empty_name_handling",
            response.status_code == 422,
            f"Empty name properly rejected: {response.status_code}",
        )

        # Test 2: Invalid ratings
        response = validator.client.post(
            "/api/vote",
            json={
                "voter_name": "Edge Case User",
                "ratings": {"toveco1.png": 5},  # Out of range
            },
        )

        validator.validate_step(
            "invalid_rating_handling",
            response.status_code == 422,
            f"Invalid rating properly rejected: {response.status_code}",
        )

        # Test 3: Missing logos
        response = validator.client.post(
            "/api/vote",
            json={
                "voter_name": "Incomplete User",
                "ratings": {"toveco1.png": 1},  # Missing other logos
            },
        )

        validator.validate_step(
            "incomplete_vote_handling",
            response.status_code == 400,  # ValidationError
            f"Incomplete vote properly rejected: {response.status_code}",
        )

        # Test 4: Extra logos
        complete_ratings = {f"toveco{i}.png": 1 for i in range(1, 12)}
        complete_ratings["invalid_logo.png"] = 1

        response = validator.client.post(
            "/api/vote",
            json={"voter_name": "Extra Logos User", "ratings": complete_ratings},
        )

        validator.validate_step(
            "extra_logo_handling",
            response.status_code == 400,  # ValidationError
            f"Extra logos properly rejected: {response.status_code}",
        )

        # Test 5: Very long name
        response = validator.client.post(
            "/api/vote",
            json={
                "voter_name": "x" * 150,
                "ratings": {f"toveco{i}.png": 1 for i in range(1, 12)},
            },
        )

        validator.validate_step(
            "long_name_handling",
            response.status_code == 422,
            f"Long name properly rejected: {response.status_code}",
        )

        # Workflow summary
        summary = validator.get_workflow_summary()
        print("\nEdge Case Workflow Summary:")
        print(f"  Steps passed: {summary['passed_steps']}/{summary['total_steps']}")
        print(f"  Success rate: {summary['success_rate']:.1f}%")

        assert summary["success_rate"] >= 90, (
            f"Edge case handling success rate too low: {summary['success_rate']:.1f}%"
        )


class TestDataConsistency:
    """Test data consistency across the entire system."""

    @pytest.fixture
    def client(self):
        """Create test client."""
        with TestClient(app) as test_client:
            yield test_client

    def test_vote_data_consistency(self, client):
        """Test that vote data remains consistent throughout the system."""
        # Submit a known vote
        test_vote = {
            "voter_name": "Consistency Test User",
            "ratings": {
                "toveco1.png": 2,
                "toveco2.png": -1,
                "toveco3.png": 0,
                "toveco4.png": 1,
                "toveco5.png": -2,
                "toveco6.png": 1,
                "toveco7.png": 0,
                "toveco8.png": 2,
                "toveco9.png": -1,
                "toveco10.png": 1,
                "toveco11.png": 0,
            },
        }

        # Submit vote
        response = client.post("/api/vote", json=test_vote)

        if response.status_code != 200:
            pytest.skip("Vote submission failed in test environment")

        vote_response = response.json()
        assert vote_response.get("success") is True
        vote_response.get("vote_id")

        # Get results
        response = client.get("/api/results")
        assert response.status_code == 200

        results_data = response.json()

        # Verify vote is reflected in results
        assert results_data["total_voters"] >= 1

        # Check that all logos from our vote appear in results
        our_logos = set(test_vote["ratings"].keys())
        result_logos = set(results_data["summary"].keys())

        # Our logos should be a subset of result logos
        assert our_logos.issubset(result_logos), (
            f"Missing logos in results: {our_logos - result_logos}"
        )

        # Verify statistics make sense
        for logo, _rating in test_vote["ratings"].items():
            if logo in results_data["summary"]:
                logo_stats = results_data["summary"][logo]

                # Stats should be reasonable
                assert logo_stats["total_votes"] >= 1
                assert -2 <= logo_stats["average"] <= 2
                assert logo_stats["ranking"] >= 1

    def test_results_calculation_consistency(self, client):
        """Test that results calculation is mathematically consistent."""
        # Submit multiple votes with known values
        test_votes = [
            {
                "voter_name": "Math User 1",
                "ratings": {"toveco1.png": 2, "toveco2.png": -1},
            },
            {
                "voter_name": "Math User 2",
                "ratings": {"toveco1.png": 1, "toveco2.png": 0},
            },
            {
                "voter_name": "Math User 3",
                "ratings": {"toveco1.png": -1, "toveco2.png": 1},
            },
        ]

        successful_submissions = 0

        for vote in test_votes:
            response = client.post("/api/vote", json=vote)
            if response.status_code == 200:
                vote_response = response.json()
                if vote_response.get("success"):
                    successful_submissions += 1

        if successful_submissions == 0:
            pytest.skip("No votes could be submitted in test environment")

        # Get results
        response = client.get("/api/results")
        assert response.status_code == 200

        results_data = response.json()

        # If all 3 votes succeeded, we can verify exact calculations
        if successful_submissions == 3:
            # toveco1: (2 + 1 + (-1)) / 3 = 2/3 â 0.67
            if "toveco1.png" in results_data["summary"]:
                logo1_stats = results_data["summary"]["toveco1.png"]
                expected_avg = 2 / 3
                actual_avg = logo1_stats["average"]

                assert abs(actual_avg - expected_avg) < 0.01, (
                    f"toveco1 average wrong: expected {expected_avg:.2f}, got {actual_avg}"
                )

            # toveco2: (-1 + 0 + 1) / 3 = 0
            if "toveco2.png" in results_data["summary"]:
                logo2_stats = results_data["summary"]["toveco2.png"]
                expected_avg = 0.0
                actual_avg = logo2_stats["average"]

                assert abs(actual_avg - expected_avg) < 0.01, (
                    f"toveco2 average wrong: expected {expected_avg:.2f}, got {actual_avg}"
                )


class TestSystemIntegration:
    """Test integration between different system components."""

    @pytest.fixture
    def client(self):
        """Create test client."""
        with TestClient(app) as test_client:
            yield test_client

    def test_api_frontend_integration(self, client):
        """Test integration between API and frontend."""
        # Test that all required API endpoints are available for frontend
        required_endpoints = [
            ("/api/health", "GET"),
            ("/api/logos", "GET"),
            ("/api/stats", "GET"),
            ("/api/results", "GET"),
            ("/api/vote", "POST"),
        ]

        for endpoint, method in required_endpoints:
            if method == "GET":
                response = client.get(endpoint)
            elif method == "POST":
                # Use minimal valid data for POST test
                test_data = {
                    "voter_name": "API Test",
                    "ratings": {f"toveco{i}.png": 0 for i in range(1, 12)},
                }
                response = client.post(endpoint, json=test_data)

            # API should respond (may not be 200 in all test environments)
            assert response.status_code < 500, (
                f"Server error on {method} {endpoint}: {response.status_code}"
            )

            # Content type should be JSON for API endpoints
            if response.status_code == 200:
                assert "application/json" in response.headers.get("content-type", "")

    def test_static_file_integration(self, client):
        """Test that static files are properly served."""
        # Test JavaScript file
        response = client.get("/static/app.js")
        assert response.status_code in [
            200,
            404,
        ], f"Unexpected status for app.js: {response.status_code}"

        # Test logo files (if available)
        response = client.get("/logos/toveco1.png")
        if response.status_code == 200:
            assert "image" in response.headers.get("content-type", "").lower()

    def test_template_integration(self, client):
        """Test that templates are properly rendered."""
        # Test main page
        response = client.get("/")
        assert response.status_code == 200
        assert "text/html" in response.headers.get("content-type", "")

        # Test results page
        response = client.get("/results")
        assert response.status_code == 200
        assert "text/html" in response.headers.get("content-type", "")


class TestBusinessLogicValidation:
    """Test business logic and voting methodology validation."""

    @pytest.fixture
    def client(self):
        """Create test client."""
        with TestClient(app) as test_client:
            yield test_client

    def test_value_voting_methodology(self, client):
        """Test that the value voting methodology (-2 to +2) is properly enforced."""
        # Test valid ratings
        valid_ratings = [-2, -1, 0, 1, 2]

        for rating in valid_ratings:
            vote_data = {
                "voter_name": f"Valid Rating Test {rating}",
                "ratings": {f"toveco{i}.png": rating for i in range(1, 12)},
            }

            response = client.post("/api/vote", json=vote_data)

            # Should either succeed or fail for environment reasons, not validation
            if response.status_code == 422:
                # If it fails validation, check it's not due to rating values
                error_data = response.json()
                error_message = str(error_data).lower()
                assert "rating" not in error_message or "range" not in error_message, (
                    f"Valid rating {rating} was rejected"
                )

        # Test invalid ratings
        invalid_ratings = [-3, 3, 5, -10]

        for rating in invalid_ratings:
            vote_data = {
                "voter_name": f"Invalid Rating Test {rating}",
                "ratings": {"toveco1.png": rating},
            }

            response = client.post("/api/vote", json=vote_data)
            assert response.status_code == 422, f"Invalid rating {rating} was accepted"

    def test_voting_completeness_requirement(self, client):
        """Test that votes must include all logos."""
        # Test incomplete vote (missing logos)
        incomplete_vote = {
            "voter_name": "Incomplete Vote Test",
            "ratings": {"toveco1.png": 1},  # Missing other 10 logos
        }

        response = client.post("/api/vote", json=incomplete_vote)
        assert response.status_code == 400, "Incomplete vote was accepted"

        # Test extra logos
        extra_vote = {
            "voter_name": "Extra Vote Test",
            "ratings": {
                **{f"toveco{i}.png": 1 for i in range(1, 12)},
                "extra_logo.png": 1,
            },
        }

        response = client.post("/api/vote", json=extra_vote)
        assert response.status_code == 400, "Vote with extra logos was accepted"

    def test_ranking_algorithm_validation(self, client):
        """Test that ranking algorithm works correctly."""
        # Submit votes with clear ranking order
        ranking_votes = [
            {
                "voter_name": "Ranking Test User",
                "ratings": {
                    "toveco1.png": 2,  # Should rank highest
                    "toveco2.png": 1,  # Should rank second
                    "toveco3.png": 0,  # Should rank third
                    "toveco4.png": -1,  # Should rank fourth
                    "toveco5.png": -2,  # Should rank lowest among these
                    **{f"toveco{i}.png": 0 for i in range(6, 12)},  # Neutral for others
                },
            }
        ]

        successful_submissions = 0
        for vote in ranking_votes:
            response = client.post("/api/vote", json=vote)
            if response.status_code == 200:
                vote_response = response.json()
                if vote_response.get("success"):
                    successful_submissions += 1

        if successful_submissions == 0:
            pytest.skip("Could not submit ranking test vote")

        # Get results and verify rankings
        response = client.get("/api/results")
        assert response.status_code == 200

        results_data = response.json()
        summary = results_data.get("summary", {})

        if len(summary) >= 5:
            # Check that logos with higher ratings have better rankings
            test_logos = [
                "toveco1.png",
                "toveco2.png",
                "toveco3.png",
                "toveco4.png",
                "toveco5.png",
            ]

            rankings = {}
            averages = {}

            for logo in test_logos:
                if logo in summary:
                    rankings[logo] = summary[logo]["ranking"]
                    averages[logo] = summary[logo]["average"]

            # Verify ranking order matches average order
            if len(rankings) >= 2:
                sorted_by_avg = sorted(
                    rankings.keys(), key=lambda x: averages[x], reverse=True
                )
                sorted_by_rank = sorted(rankings.keys(), key=lambda x: rankings[x])

                # The order should match (higher average = better ranking = lower ranking number)
                assert sorted_by_avg == sorted_by_rank, (
                    f"Ranking order doesn't match average order: {sorted_by_avg} vs {sorted_by_rank}"
                )


# Integration test runner
def run_end_to_end_tests():
    """Run end-to-end tests manually."""
    print("Running End-to-End Workflow Tests")
    print("=" * 50)

    try:
        with TestClient(app) as client:
            validator = WorkflowValidator(client)

            # Test 1: Single user workflow
            print("\n1. Testing single user workflow...")
            test = TestCompleteVotingWorkflow()
            test.test_single_user_complete_workflow(validator)

            # Test 2: Multiple users workflow
            print("\n2. Testing multiple users workflow...")
            test.test_multiple_users_workflow(validator)

            # Test 3: Edge cases
            print("\n3. Testing edge case workflow...")
            test.test_edge_case_workflow(validator)

            print("\nâ All end-to-end tests completed successfully!")

    except Exception as e:
        print(f"\nâ End-to-end tests failed: {e}")
        raise


if __name__ == "__main__":
    # Can be run standalone
    run_end_to_end_tests()

    # Or with pytest
    pytest.main([__file__, "-v"])
